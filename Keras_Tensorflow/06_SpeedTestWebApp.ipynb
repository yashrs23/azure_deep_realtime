{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test deployed web application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook pulls some images and tests them against the deployed web application. We submit requests asychronously which should reduce the contribution of latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from timeit import default_timer\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.core.webservice import AksWebservice\n",
    "from azureml.core.workspace import Workspace\n",
    "from dotenv import get_key, find_dotenv\n",
    "from testing_utilities import to_img, gen_variations_of_one_image, get_auth\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(auth=get_auth())\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrive the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service_name = get_key(env_path, 'aks_service_name')\n",
    "aks_service = AksWebservice(ws, name=aks_service_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test our service concurrently but only have 4 concurrent requests at any time. We have only deployed one pod on one node and increasing the number of concurrent calls does not really increase throughput. Feel free to try different values and see how the service responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENT_REQUESTS = 4   # Number of requests at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scoring URL and API key of the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url = aks_service.scoring_uri\n",
    "api_key = aks_service.get_keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGEURL = \"https://bostondata.blob.core.windows.net/aksdeploymenttutorialaml/220px-Lynx_lynx_poing.jpg\"\n",
    "plt.imshow(to_img(IMAGEURL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to use [Locust](https://locust.io/) to load test our deployed model. First we need to write the locustfile. We will use variations of the same image to test the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile locustfile.py\n",
    "from locust import HttpLocust, TaskSet, task\n",
    "from testing_utilities import gen_variations_of_one_image\n",
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "_IMAGEURL = os.getenv('IMAGEURL', \"https://bostondata.blob.core.windows.net/aksdeploymenttutorialaml/220px-Lynx_lynx_poing.jpg\")\n",
    "_NUMBER_OF_VARIATIONS = os.getenv('NUMBER_OF_VARIATIONS', 100)\n",
    "_SCORE_PATH = os.getenv('SCORE_PATH', \"/score\")\n",
    "_API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "\n",
    "class UserBehavior(TaskSet):\n",
    "    def on_start(self):\n",
    "        print('Running setup')\n",
    "        self._image_generator =  cycle(gen_variations_of_one_image(_IMAGEURL, _NUMBER_OF_VARIATIONS))\n",
    "        self._headers = {'Authorization':('Bearer {}'.format(_API_KEY))}\n",
    "        \n",
    "    @task\n",
    "    def score(self):\n",
    "        self.client.post(_SCORE_PATH, files={'image': next(self._image_generator)}, headers=self._headers)\n",
    "\n",
    "\n",
    "class WebsiteUser(HttpLocust):\n",
    "    task_set = UserBehavior\n",
    "    # min and max time to wait before repeating task\n",
    "    min_wait = 10\n",
    "    max_wait = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the locust command we want to run. We are going to run at a hatch rate of 10 and the whole test will last 1 minute. Feel free to adjust the parameters below and see how the results differ. The results of the test will be saved to two csv files **modeltest_requests.csv** and **modeltest_distribution.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_url = urlparse(scoring_url)\n",
    "cmd = \"locust -H {host} --no-web -c {users} -r {rate} -t {duration} --csv=modeltest --only-summary\".format(\n",
    "    host=\"{url.scheme}://{url.netloc}\".format(url=parsed_url),\n",
    "    users=CONCURRENT_REQUESTS,  # concurrent users\n",
    "    rate=10,                    # hatch rate (users / second)\n",
    "    duration='1m',              # test duration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! API_KEY={api_key} SCORE_PATH={parsed_url.path} PYTHONPATH={os.path.abspath('../')} {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the summary results of our test and below that the distribution infromation of those tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"modeltest_requests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"modeltest_distribution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tear down the cluster and all related resources go to the [tear down the cluster](07_TearDown.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
